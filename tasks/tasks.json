{
  "tasks": [
    {
      "id": 1,
      "title": "Setup Project Structure",
      "description": "Create the initial project structure with all required directories and files according to the PRD specifications.",
      "details": "Create the following directory structure:\n\n```\nncaa_basketball_pipeline/\n├── dlt_sources/         # Directory for dlt source definitions\n│   ├── __init__.py\n│   └── espn_api_source.py    # Defines @dlt.source for ESPN API\n│\n├── ncaa_basketball_pipeline/     # Dagster-specific code for the Bronze layer\n│   ├── __init__.py\n│   ├── assets.py             # Defines @dlt_assets using sources from dlt_sources\n│   └── definitions.py        # Dagster Definitions object, including DagsterDltResource\n│\n├── notebooks/                # Jupyter notebooks for exploration\n│\n├── tests/                    # Unit and integration tests\n│   ├── dlt_sources/\n│   └── ncaa_basketball_pipeline/\n│\n├── .env                      # Optional: For local development environment variables\n├── dagster.yaml              # Dagster instance configuration\n├── pyproject.toml            # Python project configuration\n└── README.md                 # Project overview\n```\n\nInitialize the project with a pyproject.toml file that includes dependencies for dagster, dagster-dlt, dlt, and other required packages.",
      "testStrategy": "Verify that all directories and files are created with the correct structure. Ensure that the pyproject.toml file includes all necessary dependencies.",
      "priority": "high",
      "dependencies": [],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Configure Development Environment",
      "description": "Set up the development environment with all required dependencies and configurations.",
      "details": "1. Create a virtual environment for the project\n2. Install all dependencies specified in pyproject.toml\n3. Configure dagster.yaml with appropriate settings\n4. Set up .env file with required environment variables for local development\n5. Configure DuckDB connection settings\n\nExample pyproject.toml content:\n```toml\n[build-system]\nrequires = [\"setuptools>=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ncaa_basketball_pipeline\"\nversion = \"0.1.0\"\ndescription = \"NCAA Basketball Data Pipeline - Bronze Layer with Dagster and dlt\"\nrequires-python = \">=3.8\"\ndependencies = [\n    \"dagster\",\n    \"dagster-webserver\",\n    \"dagster-dlt\",\n    \"dlt\",\n    \"duckdb\",\n    \"pandas\",\n    \"requests\",\n    \"python-dotenv\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest\",\n    \"black\",\n    \"isort\",\n    \"mypy\",\n    \"jupyter\",\n]\n```\n\nExample .env file:\n```\n# DuckDB Configuration\nDESTINATION__DUCKDB__CREDENTIALS__DATABASE=\"ncaa_basketball.duckdb\"\n\n# ESPN API Configuration\nSOURCES__ESPN_API_SOURCE__BASE_URL=\"https://site.api.espn.com/apis/site/v2/sports/basketball/mens-college-basketball\"\n```",
      "testStrategy": "Verify that the virtual environment is created successfully and all dependencies are installed. Test that Dagster can be started and that the DuckDB connection works.",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Explore ESPN API Endpoints",
      "description": "Explore and document the ESPN API endpoints for NCAA Men's College Basketball to understand the data structure and available endpoints.",
      "details": "Create a Jupyter notebook in the notebooks/ directory to explore the ESPN API endpoints. Document the following:\n\n1. Base URL: https://site.api.espn.com/apis/site/v2/sports/basketball/mens-college-basketball\n2. Key endpoints to explore:\n   - /seasons: List of available seasons\n   - /seasons/{year}/types: Season types (regular, postseason)\n   - /seasons/{year}/types/{type}/weeks: Weeks within a season type\n   - /scoreboard: Current scoreboard data\n   - /scoreboard?dates={YYYYMMDD}: Scoreboard for specific date\n   - /summary?event={eventId}: Detailed event summary\n   - /teams: Team information\n   - /teams/{teamId}: Specific team details\n   - /teams/{teamId}/roster: Team roster\n\n3. Document the response structure for each endpoint, focusing on:\n   - Primary keys and identifiers\n   - Nested data structures\n   - $ref links that need to be followed\n   - Pagination mechanisms if present\n\n4. Create sample requests and save example responses for testing purposes.",
      "testStrategy": "Verify that all key endpoints are accessible and document any rate limiting or access issues. Ensure that the response structures are well-documented for implementation.",
      "priority": "high",
      "dependencies": [
        2
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Define ESPN API Source in dlt",
      "description": "Create the dlt source definition for the ESPN API in the dlt_sources module.",
      "details": "Create the espn_api_source.py file with the main source definition:\n\n```python\nimport dlt\nimport requests\nfrom typing import Dict, List, Optional, Any\n\n@dlt.source\ndef espn_ncaa_basketball_source(partition_date: Optional[str] = None):\n    \"\"\"Source for ESPN NCAA Men's College Basketball data.\n    \n    Args:\n        partition_date: Optional date string in YYYY-MM-DD format for partitioned loads\n    \"\"\"\n    base_url = dlt.config.value(name=\"base_url\")\n    \n    # Define resources for different endpoints\n    @dlt.resource(\n        primary_key=[\"id\"],\n        write_disposition=\"merge\"\n    )\n    def seasons():\n        \"\"\"Fetch available seasons\"\"\"\n        response = requests.get(f\"{base_url}/seasons\")\n        response.raise_for_status()\n        return response.json()[\"seasons\"]\n    \n    @dlt.resource(\n        primary_key=[\"id\", \"season_id\"],\n        write_disposition=\"merge\"\n    )\n    def season_types(season_id: str):\n        \"\"\"Fetch season types for a specific season\"\"\"\n        response = requests.get(f\"{base_url}/seasons/{season_id}/types\")\n        response.raise_for_status()\n        return response.json()[\"types\"]\n    \n    @dlt.resource(\n        primary_key=[\"id\", \"season_id\", \"season_type\"],\n        write_disposition=\"merge\"\n    )\n    def weeks(season_id: str, season_type: str):\n        \"\"\"Fetch weeks for a specific season and type\"\"\"\n        response = requests.get(f\"{base_url}/seasons/{season_id}/types/{season_type}/weeks\")\n        response.raise_for_status()\n        return response.json()[\"weeks\"]\n    \n    @dlt.resource(\n        primary_key=[\"id\"],\n        write_disposition=\"merge\"\n    )\n    def events(date: Optional[str] = None):\n        \"\"\"Fetch events/games, optionally filtered by date\"\"\"\n        url = f\"{base_url}/scoreboard\"\n        params = {}\n        if date:\n            params[\"dates\"] = date.replace(\"-\", \"\")\n        if partition_date:\n            params[\"dates\"] = partition_date.replace(\"-\", \"\")\n            \n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        data = response.json()\n        return data.get(\"events\", [])\n    \n    @dlt.resource(\n        primary_key=[\"id\", \"event_id\"],\n        write_disposition=\"merge\"\n    )\n    def event_competitors(event_id: str, event_data: Dict):\n        \"\"\"Extract competitors from event data\"\"\"\n        competitors = []\n        for competitor in event_data.get(\"competitions\", [{}])[0].get(\"competitors\", []):\n            competitor[\"event_id\"] = event_id\n            competitors.append(competitor)\n        return competitors\n    \n    @dlt.resource(\n        primary_key=[\"id\", \"team_id\"],\n        write_disposition=\"merge\"\n    )\n    def teams():\n        \"\"\"Fetch team information\"\"\"\n        response = requests.get(f\"{base_url}/teams\")\n        response.raise_for_status()\n        return response.json().get(\"sports\", [{}])[0].get(\"leagues\", [{}])[0].get(\"teams\", [])\n    \n    # Add more resources for other endpoints\n    # ...\n    \n    # Return all resources\n    return {\n        \"seasons\": seasons,\n        \"season_types\": season_types,\n        \"weeks\": weeks,\n        \"events\": events,\n        \"event_competitors\": event_competitors,\n        \"teams\": teams,\n        # Add more resources\n    }\n```\n\nThis is a starting point. Additional resources should be added for all the entities mentioned in the PRD's Conceptual Data Model section.",
      "testStrategy": "Write unit tests that mock API responses and verify that the dlt source correctly extracts and structures the data. Test with sample responses collected during API exploration.",
      "priority": "high",
      "dependencies": [
        3
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Implement Event Details Resource",
      "description": "Extend the ESPN API source with a resource for fetching detailed event information.",
      "details": "Add a new resource to the espn_ncaa_basketball_source for fetching detailed event information:\n\n```python\n@dlt.resource(\n    primary_key=[\"id\"],\n    write_disposition=\"merge\"\n)\ndef event_details(event_id: str):\n    \"\"\"Fetch detailed information for a specific event\"\"\"\n    response = requests.get(f\"{base_url}/summary?event={event_id}\")\n    response.raise_for_status()\n    return response.json()\n```\n\nThen update the source function to include this resource:\n\n```python\n# Return all resources\nreturn {\n    \"seasons\": seasons,\n    \"season_types\": season_types,\n    \"weeks\": weeks,\n    \"events\": events,\n    \"event_competitors\": event_competitors,\n    \"teams\": teams,\n    \"event_details\": event_details,\n    # Add more resources\n}\n```\n\nImplement logic to handle $ref links in the API responses, which may require additional requests to fetch referenced data.",
      "testStrategy": "Test the event_details resource with mock API responses. Verify that it correctly handles the response structure and extracts all relevant information.",
      "priority": "medium",
      "dependencies": [
        4
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Implement Team and Roster Resources",
      "description": "Extend the ESPN API source with resources for fetching team details and rosters.",
      "details": "Add new resources to the espn_ncaa_basketball_source for fetching team details and rosters:\n\n```python\n@dlt.resource(\n    primary_key=[\"id\"],\n    write_disposition=\"merge\"\n)\ndef team_details(team_id: str):\n    \"\"\"Fetch detailed information for a specific team\"\"\"\n    response = requests.get(f\"{base_url}/teams/{team_id}\")\n    response.raise_for_status()\n    return response.json().get(\"team\", {})\n\n@dlt.resource(\n    primary_key=[\"id\", \"team_id\"],\n    write_disposition=\"merge\"\n)\ndef team_roster(team_id: str):\n    \"\"\"Fetch roster for a specific team\"\"\"\n    response = requests.get(f\"{base_url}/teams/{team_id}/roster\")\n    response.raise_for_status()\n    athletes = response.json().get(\"athletes\", [])\n    for athlete in athletes:\n        athlete[\"team_id\"] = team_id\n    return athletes\n```\n\nThen update the source function to include these resources:\n\n```python\n# Return all resources\nreturn {\n    \"seasons\": seasons,\n    \"season_types\": season_types,\n    \"weeks\": weeks,\n    \"events\": events,\n    \"event_competitors\": event_competitors,\n    \"teams\": teams,\n    \"event_details\": event_details,\n    \"team_details\": team_details,\n    \"team_roster\": team_roster,\n    # Add more resources\n}\n```",
      "testStrategy": "Test the team_details and team_roster resources with mock API responses. Verify that they correctly handle the response structure and extract all relevant information.",
      "priority": "medium",
      "dependencies": [
        4
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Implement Statistics Resources",
      "description": "Extend the ESPN API source with resources for fetching team and player statistics.",
      "details": "Add new resources to the espn_ncaa_basketball_source for fetching team and player statistics:\n\n```python\n@dlt.resource(\n    primary_key=[\"event_id\", \"team_id\"],\n    write_disposition=\"merge\"\n)\ndef team_statistics(event_id: str, event_data: Dict):\n    \"\"\"Extract team statistics from event data\"\"\"\n    team_stats = []\n    for competition in event_data.get(\"competitions\", []):\n        for team in competition.get(\"competitors\", []):\n            stats = {\n                \"event_id\": event_id,\n                \"team_id\": team.get(\"id\"),\n                \"team_name\": team.get(\"team\", {}).get(\"name\")\n            }\n            # Extract statistics from team data\n            for stat in team.get(\"statistics\", []):\n                stats[stat.get(\"name\")] = stat.get(\"value\")\n            team_stats.append(stats)\n    return team_stats\n\n@dlt.resource(\n    primary_key=[\"event_id\", \"team_id\", \"athlete_id\"],\n    write_disposition=\"merge\"\n)\ndef player_statistics(event_id: str, event_data: Dict):\n    \"\"\"Extract player statistics from event data\"\"\"\n    player_stats = []\n    for competition in event_data.get(\"competitions\", []):\n        for team in competition.get(\"competitors\", []):\n            team_id = team.get(\"id\")\n            for athlete in team.get(\"athletes\", []):\n                stats = {\n                    \"event_id\": event_id,\n                    \"team_id\": team_id,\n                    \"athlete_id\": athlete.get(\"id\"),\n                    \"athlete_name\": athlete.get(\"fullName\")\n                }\n                # Extract statistics from athlete data\n                for stat in athlete.get(\"statistics\", []):\n                    stats[stat.get(\"name\")] = stat.get(\"value\")\n                player_stats.append(stats)\n    return player_stats\n```\n\nThen update the source function to include these resources:\n\n```python\n# Return all resources\nreturn {\n    \"seasons\": seasons,\n    \"season_types\": season_types,\n    \"weeks\": weeks,\n    \"events\": events,\n    \"event_competitors\": event_competitors,\n    \"teams\": teams,\n    \"event_details\": event_details,\n    \"team_details\": team_details,\n    \"team_roster\": team_roster,\n    \"team_statistics\": team_statistics,\n    \"player_statistics\": player_statistics,\n    # Add more resources\n}\n```",
      "testStrategy": "Test the team_statistics and player_statistics resources with mock API responses. Verify that they correctly extract and structure the statistics data.",
      "priority": "medium",
      "dependencies": [
        5
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 8,
      "title": "Implement Play-by-Play Resource",
      "description": "Extend the ESPN API source with a resource for fetching play-by-play data for games.",
      "details": "Add a new resource to the espn_ncaa_basketball_source for fetching play-by-play data:\n\n```python\n@dlt.resource(\n    primary_key=[\"id\", \"event_id\"],\n    write_disposition=\"merge\"\n)\ndef plays(event_id: str):\n    \"\"\"Fetch play-by-play data for a specific event\"\"\"\n    response = requests.get(f\"{base_url}/playbyplay?event={event_id}\")\n    response.raise_for_status()\n    plays_data = response.json().get(\"plays\", [])\n    for play in plays_data:\n        play[\"event_id\"] = event_id\n    return plays_data\n```\n\nThen update the source function to include this resource:\n\n```python\n# Return all resources\nreturn {\n    \"seasons\": seasons,\n    \"season_types\": season_types,\n    \"weeks\": weeks,\n    \"events\": events,\n    \"event_competitors\": event_competitors,\n    \"teams\": teams,\n    \"event_details\": event_details,\n    \"team_details\": team_details,\n    \"team_roster\": team_roster,\n    \"team_statistics\": team_statistics,\n    \"player_statistics\": player_statistics,\n    \"plays\": plays,\n    # Add more resources\n}\n```",
      "testStrategy": "Test the plays resource with mock API responses. Verify that it correctly extracts and structures the play-by-play data.",
      "priority": "medium",
      "dependencies": [
        5
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "Implement Venues and Records Resources",
      "description": "Extend the ESPN API source with resources for fetching venue information and team records.",
      "details": "Add new resources to the espn_ncaa_basketball_source for fetching venue information and team records:\n\n```python\n@dlt.resource(\n    primary_key=[\"id\"],\n    write_disposition=\"merge\"\n)\ndef venues(event_data: Dict):\n    \"\"\"Extract venue information from event data\"\"\"\n    venues = []\n    for competition in event_data.get(\"competitions\", []):\n        venue = competition.get(\"venue\", {})\n        if venue and \"id\" in venue:\n            venues.append(venue)\n    return venues\n\n@dlt.resource(\n    primary_key=[\"team_id\", \"type\", \"season_id\"],\n    write_disposition=\"merge\"\n)\ndef team_records(team_id: str, season_id: Optional[str] = None):\n    \"\"\"Fetch records for a specific team\"\"\"\n    url = f\"{base_url}/teams/{team_id}/record\"\n    params = {}\n    if season_id:\n        params[\"season\"] = season_id\n        \n    response = requests.get(url, params=params)\n    response.raise_for_status()\n    records = []\n    for record in response.json().get(\"records\", []):\n        record_data = {\n            \"team_id\": team_id,\n            \"type\": record.get(\"type\"),\n            \"summary\": record.get(\"summary\"),\n            \"wins\": record.get(\"wins\"),\n            \"losses\": record.get(\"losses\"),\n            \"ties\": record.get(\"ties\", 0),\n            \"season_id\": season_id or \"current\"\n        }\n        records.append(record_data)\n    return records\n```\n\nThen update the source function to include these resources:\n\n```python\n# Return all resources\nreturn {\n    \"seasons\": seasons,\n    \"season_types\": season_types,\n    \"weeks\": weeks,\n    \"events\": events,\n    \"event_competitors\": event_competitors,\n    \"teams\": teams,\n    \"event_details\": event_details,\n    \"team_details\": team_details,\n    \"team_roster\": team_roster,\n    \"team_statistics\": team_statistics,\n    \"player_statistics\": player_statistics,\n    \"plays\": plays,\n    \"venues\": venues,\n    \"team_records\": team_records,\n    # Add more resources\n}\n```",
      "testStrategy": "Test the venues and team_records resources with mock API responses. Verify that they correctly extract and structure the data.",
      "priority": "medium",
      "dependencies": [
        4
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 10,
      "title": "Implement Leaders and Scores Resources",
      "description": "Extend the ESPN API source with resources for fetching game leaders and detailed scores.",
      "details": "Add new resources to the espn_ncaa_basketball_source for fetching game leaders and detailed scores:\n\n```python\n@dlt.resource(\n    primary_key=[\"event_id\", \"team_id\", \"category\"],\n    write_disposition=\"merge\"\n)\ndef leaders(event_id: str, event_data: Dict):\n    \"\"\"Extract leader information from event data\"\"\"\n    leaders_data = []\n    for competition in event_data.get(\"competitions\", []):\n        for leader_category in competition.get(\"leaders\", []):\n            category = leader_category.get(\"name\")\n            for team_leader in leader_category.get(\"leaders\", []):\n                for athlete in team_leader.get(\"athletes\", []):\n                    leader = {\n                        \"event_id\": event_id,\n                        \"team_id\": team_leader.get(\"team\", {}).get(\"id\"),\n                        \"category\": category,\n                        \"athlete_id\": athlete.get(\"id\"),\n                        \"athlete_name\": athlete.get(\"fullName\"),\n                        \"value\": team_leader.get(\"value\")\n                    }\n                    leaders_data.append(leader)\n    return leaders_data\n\n@dlt.resource(\n    primary_key=[\"event_id\", \"team_id\", \"period_number\"],\n    write_disposition=\"merge\"\n)\ndef linescores(event_id: str, event_data: Dict):\n    \"\"\"Extract period-by-period scores from event data\"\"\"\n    linescores_data = []\n    for competition in event_data.get(\"competitions\", []):\n        for competitor in competition.get(\"competitors\", []):\n            team_id = competitor.get(\"id\")\n            for period, score in enumerate(competitor.get(\"linescores\", []), 1):\n                linescore = {\n                    \"event_id\": event_id,\n                    \"team_id\": team_id,\n                    \"period_number\": period,\n                    \"score\": score.get(\"value\")\n                }\n                linescores_data.append(linescore)\n    return linescores_data\n```\n\nThen update the source function to include these resources:\n\n```python\n# Return all resources\nreturn {\n    \"seasons\": seasons,\n    \"season_types\": season_types,\n    \"weeks\": weeks,\n    \"events\": events,\n    \"event_competitors\": event_competitors,\n    \"teams\": teams,\n    \"event_details\": event_details,\n    \"team_details\": team_details,\n    \"team_roster\": team_roster,\n    \"team_statistics\": team_statistics,\n    \"player_statistics\": player_statistics,\n    \"plays\": plays,\n    \"venues\": venues,\n    \"team_records\": team_records,\n    \"leaders\": leaders,\n    \"linescores\": linescores,\n    # Add more resources\n}\n```",
      "testStrategy": "Test the leaders and linescores resources with mock API responses. Verify that they correctly extract and structure the data.",
      "priority": "medium",
      "dependencies": [
        5
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 11,
      "title": "Implement Error Handling and Retry Logic",
      "description": "Enhance the ESPN API source with robust error handling and retry logic to handle API failures and rate limiting.",
      "details": "Implement a retry decorator and error handling for all API requests in the ESPN API source:\n\n```python\nimport time\nfrom functools import wraps\nfrom requests.exceptions import RequestException\n\ndef retry_with_backoff(max_retries=3, initial_backoff=1, backoff_factor=2):\n    \"\"\"Decorator for retrying API requests with exponential backoff\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            retries = 0\n            backoff = initial_backoff\n            while retries <= max_retries:\n                try:\n                    return func(*args, **kwargs)\n                except RequestException as e:\n                    retries += 1\n                    if retries > max_retries:\n                        raise\n                    # Check for rate limiting (HTTP 429)\n                    if hasattr(e, 'response') and e.response is not None and e.response.status_code == 429:\n                        # Use retry-after header if available\n                        retry_after = int(e.response.headers.get('Retry-After', backoff))\n                        time.sleep(retry_after)\n                    else:\n                        # Exponential backoff for other errors\n                        time.sleep(backoff)\n                        backoff *= backoff_factor\n        return wrapper\n    return decorator\n\n# Apply the decorator to a helper function for making API requests\n@retry_with_backoff()\ndef make_api_request(url, params=None):\n    \"\"\"Make an API request with retry logic\"\"\"\n    response = requests.get(url, params=params)\n    response.raise_for_status()\n    return response.json()\n```\n\nThen update all resource functions to use this helper function instead of direct requests.get() calls. For example:\n\n```python\n@dlt.resource(\n    primary_key=[\"id\"],\n    write_disposition=\"merge\"\n)\ndef seasons():\n    \"\"\"Fetch available seasons\"\"\"\n    data = make_api_request(f\"{base_url}/seasons\")\n    return data[\"seasons\"]\n```\n\nAlso, add comprehensive error handling within each resource function to handle unexpected data structures or missing fields.",
      "testStrategy": "Test the retry logic with mocked API responses that simulate rate limiting (HTTP 429) and other common errors. Verify that the backoff mechanism works correctly and that the code eventually succeeds or fails appropriately.",
      "priority": "high",
      "dependencies": [
        4,
        5,
        6,
        7,
        8,
        9,
        10
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 12,
      "title": "Implement Pagination Handling",
      "description": "Enhance the ESPN API source to handle pagination for endpoints that return large datasets.",
      "details": "Implement a generic pagination handler for ESPN API endpoints that support pagination:\n\n```python\ndef paginated_request(url, params=None, limit=None):\n    \"\"\"Make paginated API requests\"\"\"\n    if params is None:\n        params = {}\n    \n    all_results = []\n    page = 1\n    more_pages = True\n    \n    while more_pages:\n        # Add pagination parameters\n        page_params = params.copy()\n        page_params['page'] = page\n        \n        # Make the request\n        response_data = make_api_request(url, page_params)\n        \n        # Extract results (adjust based on actual API response structure)\n        page_results = response_data.get('items', [])\n        all_results.extend(page_results)\n        \n        # Check if there are more pages\n        more_pages = response_data.get('hasMore', False)\n        \n        # Increment page counter\n        page += 1\n        \n        # Check if we've reached the optional limit\n        if limit and len(all_results) >= limit:\n            all_results = all_results[:limit]\n            break\n    \n    return all_results\n```\n\nThen update relevant resource functions to use this pagination handler for endpoints that might return large datasets. For example:\n\n```python\n@dlt.resource(\n    primary_key=[\"id\"],\n    write_disposition=\"merge\"\n)\ndef teams(limit=None):\n    \"\"\"Fetch team information with pagination support\"\"\"\n    return paginated_request(f\"{base_url}/teams\", limit=limit)\n```\n\nNote: The actual pagination mechanism might differ based on the ESPN API's implementation. Adjust the `paginated_request` function accordingly after exploring the API's pagination behavior.",
      "testStrategy": "Test the pagination handler with mock API responses that simulate multiple pages of data. Verify that it correctly fetches and combines all pages of data.",
      "priority": "medium",
      "dependencies": [
        4,
        5,
        6,
        7,
        8,
        9,
        10
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Define Dagster DLT Resource",
      "description": "Create the Dagster DLT resource definition for integrating dlt with Dagster.",
      "details": "Create the definitions.py file in the ncaa_basketball_pipeline directory to define the Dagster DLT resource:\n\n```python\nfrom dagster import Definitions\nfrom dagster_embedded_elt.dlt import DagsterDltResource\n\nfrom .assets import espn_bronze_assets_definition, espn_bronze_partitioned_assets_definition\n\ndlt_resource = DagsterDltResource()\n\ndefs = Definitions(\n    assets=[espn_bronze_assets_definition, espn_bronze_partitioned_assets_definition],\n    resources={\n        \"dlt_resource\": dlt_resource,\n    }\n)\n```\n\nThis defines a DagsterDltResource that will be used by the assets to run dlt pipelines. The resource is added to the Dagster Definitions object along with the assets that will use it.",
      "testStrategy": "Verify that the Dagster DLT resource is correctly defined and can be loaded by Dagster. Test that it can be accessed from asset definitions.",
      "priority": "high",
      "dependencies": [
        2
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 14,
      "title": "Implement Non-Partitioned Dagster Assets",
      "description": "Create Dagster assets for the ESPN API source without partitioning for full data loads.",
      "details": "Create the assets.py file in the ncaa_basketball_pipeline directory to define the non-partitioned Dagster assets:\n\n```python\nfrom dagster import AssetExecutionContext\nfrom dagster_embedded_elt.dlt import DagsterDltResource, dlt_assets\nimport dlt\n\nfrom ..dlt_sources.espn_api_source import espn_ncaa_basketball_source\n\n@dlt_assets(\n    dlt_source=espn_ncaa_basketball_source(),\n    dlt_pipeline=dlt.pipeline(\n        pipeline_name=\"espn_bronze_pipeline\",\n        dataset_name=\"bronze\",  # This will be the schema in DuckDB\n        destination=\"duckdb\",\n        progress=\"log\"\n    ),\n    name=\"bronze_espn_assets\",\n    group_name=\"bronze_layer\"\n)\ndef espn_bronze_assets_definition(context: AssetExecutionContext, dlt_resource: DagsterDltResource):\n    \"\"\"Dagster assets for loading ESPN NCAA Basketball data into the bronze layer.\"\"\"\n    # Run the dlt pipeline\n    yield from dlt_resource.run(context=context)\n```\n\nThis defines a set of Dagster assets using the `@dlt_assets` decorator. The assets use the ESPN API source defined earlier and configure a dlt pipeline to load data into the \"bronze\" schema in DuckDB.",
      "testStrategy": "Test that the assets can be materialized successfully. Verify that data is loaded into the correct tables in the bronze schema of the DuckDB database.",
      "priority": "high",
      "dependencies": [
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        13
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Implement Partitioned Dagster Assets",
      "description": "Create partitioned Dagster assets for the ESPN API source to support incremental loads and backfills.",
      "details": "Add partitioned assets to the assets.py file in the ncaa_basketball_pipeline directory:\n\n```python\nfrom dagster import AssetExecutionContext, DailyPartitionsDefinition\nfrom dagster_embedded_elt.dlt import DagsterDltResource, dlt_assets\nimport dlt\nfrom typing import Optional\n\nfrom ..dlt_sources.espn_api_source import espn_ncaa_basketball_source\n\n# Define daily partitions starting from a reasonable date\ndaily_partitions = DailyPartitionsDefinition(start_date=\"2023-11-01\")\n\n@dlt_assets(\n    dlt_pipeline=dlt.pipeline(\n        pipeline_name=\"espn_bronze_partitioned_pipeline\",\n        dataset_name=\"bronze\",\n        destination=\"duckdb\",\n        progress=\"log\"\n    ),\n    partitions_def=daily_partitions,\n    name=\"bronze_espn_partitioned_assets\",\n    group_name=\"bronze_layer\"\n    # dlt_source is provided in the run method for partitioned assets\n)\ndef espn_bronze_partitioned_assets_definition(context: AssetExecutionContext, dlt_resource: DagsterDltResource):\n    \"\"\"Partitioned Dagster assets for loading ESPN NCAA Basketball data by date.\"\"\"\n    partition_key_str = context.partition_key\n    # Pass the partition key to the source function\n    yield from dlt_resource.run(\n        context=context,\n        dlt_source=espn_ncaa_basketball_source(partition_date=partition_key_str)\n    )\n```\n\nThis defines a set of partitioned Dagster assets using the `@dlt_assets` decorator with a `DailyPartitionsDefinition`. The assets use the ESPN API source defined earlier, passing the partition key as the `partition_date` parameter to filter data by date.",
      "testStrategy": "Test that the partitioned assets can be materialized successfully for specific partition keys. Verify that data is loaded into the correct tables in the bronze schema of the DuckDB database and is filtered correctly by date.",
      "priority": "high",
      "dependencies": [
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        13
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 16,
      "title": "Implement Dagster Schedules",
      "description": "Create Dagster schedules for automatically materializing assets based on the data freshness requirements.",
      "details": "Update the definitions.py file in the ncaa_basketball_pipeline directory to include schedules:\n\n```python\nfrom dagster import Definitions, ScheduleDefinition, define_asset_job\nfrom dagster_embedded_elt.dlt import DagsterDltResource\n\nfrom .assets import espn_bronze_assets_definition, espn_bronze_partitioned_assets_definition\n\n# Define jobs for the assets\nfull_load_job = define_asset_job(name=\"full_load_job\", selection=\"bronze_espn_assets\")\ndaily_update_job = define_asset_job(name=\"daily_update_job\", selection=\"bronze_espn_partitioned_assets\")\n\n# Define schedules\nweekly_full_load_schedule = ScheduleDefinition(\n    job=full_load_job,\n    cron_schedule=\"0 0 * * 0\",  # Weekly on Sunday at midnight\n    name=\"weekly_full_load\"\n)\n\ndaily_update_schedule = ScheduleDefinition(\n    job=daily_update_job,\n    cron_schedule=\"0 6 * * *\",  # Daily at 6 AM\n    name=\"daily_update\"\n)\n\n# Create the DLT resource\ndlt_resource = DagsterDltResource()\n\n# Define the Dagster definitions\ndefs = Definitions(\n    assets=[espn_bronze_assets_definition, espn_bronze_partitioned_assets_definition],\n    resources={\n        \"dlt_resource\": dlt_resource,\n    },\n    schedules=[weekly_full_load_schedule, daily_update_schedule]\n)\n```\n\nThis defines two schedules:\n1. A weekly full load schedule that runs every Sunday at midnight\n2. A daily update schedule that runs every day at 6 AM\n\nThe schedules are added to the Dagster Definitions object along with the assets and resources.",
      "testStrategy": "Verify that the schedules are correctly defined and can be enabled in the Dagster UI. Test that they trigger the appropriate jobs at the scheduled times.",
      "priority": "medium",
      "dependencies": [
        14,
        15
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 17,
      "title": "Implement Unit Tests for dlt Sources",
      "description": "Create unit tests for the ESPN API source functions to ensure they correctly extract and structure data.",
      "details": "Create unit tests for the ESPN API source functions in the tests/dlt_sources directory:\n\n```python\n# tests/dlt_sources/test_espn_api_source.py\nimport pytest\nimport json\nfrom unittest.mock import patch, MagicMock\nimport os\nimport sys\n\n# Add the project root to the Python path\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))\n\nfrom ncaa_basketball_pipeline.dlt_sources.espn_api_source import espn_ncaa_basketball_source\n\n# Load mock data from JSON files\ndef load_mock_data(filename):\n    with open(os.path.join(os.path.dirname(__file__), 'mock_data', filename), 'r') as f:\n        return json.load(f)\n\n# Mock API responses\n@pytest.fixture\ndef mock_seasons_response():\n    return load_mock_data('seasons.json')\n\n@pytest.fixture\ndef mock_events_response():\n    return load_mock_data('events.json')\n\n# Test the seasons resource\n@patch('ncaa_basketball_pipeline.dlt_sources.espn_api_source.make_api_request')\ndef test_seasons_resource(mock_make_api_request, mock_seasons_response):\n    # Configure the mock to return the mock data\n    mock_make_api_request.return_value = mock_seasons_response\n    \n    # Get the source\n    source = espn_ncaa_basketball_source()\n    \n    # Call the seasons resource\n    seasons = source['seasons']()\n    \n    # Verify the results\n    assert len(seasons) > 0\n    assert 'id' in seasons[0]\n    assert 'name' in seasons[0]\n    \n    # Verify the API was called correctly\n    mock_make_api_request.assert_called_once()\n\n# Test the events resource\n@patch('ncaa_basketball_pipeline.dlt_sources.espn_api_source.make_api_request')\ndef test_events_resource(mock_make_api_request, mock_events_response):\n    # Configure the mock to return the mock data\n    mock_make_api_request.return_value = mock_events_response\n    \n    # Get the source\n    source = espn_ncaa_basketball_source()\n    \n    # Call the events resource\n    events = source['events']()\n    \n    # Verify the results\n    assert len(events) > 0\n    assert 'id' in events[0]\n    \n    # Verify the API was called correctly\n    mock_make_api_request.assert_called_once()\n\n# Test the events resource with a partition date\n@patch('ncaa_basketball_pipeline.dlt_sources.espn_api_source.make_api_request')\ndef test_events_resource_with_partition(mock_make_api_request, mock_events_response):\n    # Configure the mock to return the mock data\n    mock_make_api_request.return_value = mock_events_response\n    \n    # Get the source with a partition date\n    source = espn_ncaa_basketball_source(partition_date='2023-11-01')\n    \n    # Call the events resource\n    events = source['events']()\n    \n    # Verify the results\n    assert len(events) > 0\n    \n    # Verify the API was called with the correct parameters\n    mock_make_api_request.assert_called_once()\n    args, kwargs = mock_make_api_request.call_args\n    assert 'dates=20231101' in args[0] or kwargs.get('params', {}).get('dates') == '20231101'\n```\n\nCreate a mock_data directory in tests/dlt_sources with JSON files containing sample API responses for testing.",
      "testStrategy": "Run the unit tests with pytest to verify that the ESPN API source functions correctly extract and structure data from the mock API responses.",
      "priority": "high",
      "dependencies": [
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 18,
      "title": "Implement Integration Tests for Dagster Assets",
      "description": "Create integration tests for the Dagster assets to ensure they correctly materialize data into the bronze layer.",
      "details": "Create integration tests for the Dagster assets in the tests/ncaa_basketball_pipeline directory:\n\n```python\n# tests/ncaa_basketball_pipeline/test_assets.py\nimport pytest\nimport os\nimport sys\nimport duckdb\nfrom dagster import build_init_resource_context, materialize\nfrom unittest.mock import patch\n\n# Add the project root to the Python path\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))\n\nfrom ncaa_basketball_pipeline.ncaa_basketball_pipeline.assets import espn_bronze_assets_definition\nfrom ncaa_basketball_pipeline.ncaa_basketball_pipeline.definitions import dlt_resource\n\n# Setup and teardown for a test DuckDB database\n@pytest.fixture\ndef test_duckdb_path():\n    # Use a temporary path for the test database\n    path = \"test_ncaa_basketball.duckdb\"\n    yield path\n    # Clean up after the test\n    if os.path.exists(path):\n        os.remove(path)\n\n# Mock the dlt source to return controlled test data\n@pytest.fixture\ndef mock_dlt_source():\n    with patch('ncaa_basketball_pipeline.dlt_sources.espn_api_source.espn_ncaa_basketball_source') as mock:\n        # Configure the mock to return test data\n        # This would typically involve setting up mock resources that return test data\n        yield mock\n\n# Test that the assets can be materialized\ndef test_espn_bronze_assets_materialization(test_duckdb_path, mock_dlt_source):\n    # Set the DuckDB path for the test\n    os.environ[\"DESTINATION__DUCKDB__CREDENTIALS__DATABASE\"] = test_duckdb_path\n    \n    # Initialize the dlt resource\n    init_context = build_init_resource_context()\n    resource_instance = dlt_resource.initialize(init_context)\n    \n    # Materialize the assets\n    result = materialize(\n        [espn_bronze_assets_definition],\n        resources={\"dlt_resource\": resource_instance}\n    )\n    \n    # Verify that the materialization was successful\n    assert result.success\n    \n    # Verify that data was loaded into the bronze schema\n    conn = duckdb.connect(test_duckdb_path)\n    # Check that the bronze schema exists\n    schemas = conn.execute(\"SELECT schema_name FROM information_schema.schemata\").fetchall()\n    assert ('bronze',) in schemas\n    \n    # Check that expected tables exist in the bronze schema\n    tables = conn.execute(\n        \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'bronze'\"\n    ).fetchall()\n    expected_tables = ['seasons', 'events', 'teams']  # Add more as needed\n    for table in expected_tables:\n        assert (table,) in tables\n    \n    # Clean up\n    conn.close()\n```\n\nThis test verifies that the Dagster assets can be materialized successfully and that they create the expected tables in the bronze schema of the DuckDB database.",
      "testStrategy": "Run the integration tests with pytest to verify that the Dagster assets correctly materialize data into the bronze layer of the DuckDB database.",
      "priority": "high",
      "dependencies": [
        14,
        15
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 19,
      "title": "Implement Data Validation Tests",
      "description": "Create data validation tests to ensure the quality and integrity of the data loaded into the bronze layer.",
      "details": "Create data validation tests in the tests/ncaa_basketball_pipeline directory:\n\n```python\n# tests/ncaa_basketball_pipeline/test_data_validation.py\nimport pytest\nimport os\nimport sys\nimport duckdb\nfrom dagster import materialize\n\n# Add the project root to the Python path\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))\n\nfrom ncaa_basketball_pipeline.ncaa_basketball_pipeline.assets import espn_bronze_assets_definition\nfrom ncaa_basketball_pipeline.ncaa_basketball_pipeline.definitions import dlt_resource\n\n# Setup for accessing a test DuckDB database with data\n@pytest.fixture\ndef duckdb_connection():\n    # This assumes that test_assets.py has already created and populated the test database\n    path = \"test_ncaa_basketball.duckdb\"\n    conn = duckdb.connect(path)\n    yield conn\n    conn.close()\n\n# Test that primary keys are not null\ndef test_primary_keys_not_null(duckdb_connection):\n    # Check that the id column in the seasons table is not null\n    result = duckdb_connection.execute(\n        \"SELECT COUNT(*) FROM bronze.seasons WHERE id IS NULL\"\n    ).fetchone()[0]\n    assert result == 0\n    \n    # Check that the id column in the events table is not null\n    result = duckdb_connection.execute(\n        \"SELECT COUNT(*) FROM bronze.events WHERE id IS NULL\"\n    ).fetchone()[0]\n    assert result == 0\n    \n    # Add more checks for other tables\n\n# Test that relationships between tables are valid\ndef test_relationships(duckdb_connection):\n    # Check that all event_id values in event_competitors exist in events\n    result = duckdb_connection.execute(\"\"\"\n        SELECT COUNT(*) FROM bronze.event_competitors ec\n        LEFT JOIN bronze.events e ON ec.event_id = e.id\n        WHERE e.id IS NULL\n    \"\"\").fetchone()[0]\n    assert result == 0\n    \n    # Add more relationship checks\n\n# Test that data ranges are reasonable\ndef test_data_ranges(duckdb_connection):\n    # Check that event dates are within a reasonable range\n    result = duckdb_connection.execute(\"\"\"\n        SELECT COUNT(*) FROM bronze.events\n        WHERE date < '2000-01-01' OR date > CURRENT_DATE\n    \"\"\").fetchone()[0]\n    assert result == 0\n    \n    # Add more range checks\n\n# Test that required fields are present\ndef test_required_fields(duckdb_connection):\n    # Check that all events have a name\n    result = duckdb_connection.execute(\n        \"SELECT COUNT(*) FROM bronze.events WHERE name IS NULL OR name = ''\"\n    ).fetchone()[0]\n    assert result == 0\n    \n    # Add more required field checks\n```\n\nThese tests verify the quality and integrity of the data loaded into the bronze layer, checking for null primary keys, valid relationships between tables, reasonable data ranges, and the presence of required fields.",
      "testStrategy": "Run the data validation tests with pytest to verify the quality and integrity of the data loaded into the bronze layer of the DuckDB database.",
      "priority": "medium",
      "dependencies": [
        14,
        15,
        18
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 20,
      "title": "Create Documentation and README",
      "description": "Create comprehensive documentation for the project, including a detailed README, API documentation, and usage examples.",
      "details": "Create a comprehensive README.md file at the project root:\n\n```markdown\n# NCAA Basketball Data Pipeline - Bronze Layer\n\nThis project implements a robust and scalable data pipeline for NCAA Men's College Basketball data, focusing on the Bronze layer. It uses dlt (data load tool) for data extraction and loading, deeply integrated and orchestrated as software-defined assets within the Dagster framework using the dagster-dlt library.\n\n## Architecture\n\nThe pipeline follows a specific architectural approach where dlt sources are defined as Python functions and then materialized as Dagster assets via the `@dlt_assets` decorator. Data is stored in a DuckDB database named `ncaa_basketball.duckdb`, specifically within a `bronze` schema.\n\n### Key Components\n\n- **dlt Sources**: Python functions decorated with `@dlt.source` and containing one or more `@dlt.resource` functions that encapsulate the logic for fetching data from ESPN API endpoints.\n- **Dagster Assets**: Defined using the `@dlt_assets` decorator, these assets materialize the dlt sources into the DuckDB database.\n- **Partitioning**: Dagster's partitioning is used to manage historical backfills and incremental loads.\n- **Scheduling**: Dagster schedules trigger the materialization of assets according to data freshness requirements.\n\n## Project Structure\n\n```\nncaa_basketball_pipeline/\n├── dlt_sources/         # Directory for dlt source definitions\n│   ├── __init__.py\n│   └── espn_api_source.py    # Defines @dlt.source for ESPN API\n│\n├── ncaa_basketball_pipeline/     # Dagster-specific code for the Bronze layer\n│   ├── __init__.py\n│   ├── assets.py             # Defines @dlt_assets using sources from dlt_sources\n│   └── definitions.py        # Dagster Definitions object, including DagsterDltResource\n│\n├── notebooks/                # Jupyter notebooks for exploration\n│\n├── tests/                    # Unit and integration tests\n│   ├── dlt_sources/\n│   └── ncaa_basketball_pipeline/\n│\n├── .env                      # Optional: For local development environment variables\n├── dagster.yaml              # Dagster instance configuration\n├── pyproject.toml            # Python project configuration\n└── README.md                 # Project overview\n```\n\n## Installation\n\n1. Clone the repository\n2. Create a virtual environment: `python -m venv venv`\n3. Activate the virtual environment: `source venv/bin/activate` (Linux/Mac) or `venv\\Scripts\\activate` (Windows)\n4. Install the project: `pip install -e .`\n\n## Configuration\n\nConfiguration is managed via environment variables. Create a `.env` file with the following variables:\n\n```\n# DuckDB Configuration\nDESTINATION__DUCKDB__CREDENTIALS__DATABASE=\"ncaa_basketball.duckdb\"\n\n# ESPN API Configuration\nSOURCES__ESPN_API_SOURCE__BASE_URL=\"https://site.api.espn.com/apis/site/v2/sports/basketball/mens-college-basketball\"\n```\n\n## Usage\n\n### Running the Pipeline\n\nStart the Dagster UI:\n\n```\ndagster dev\n```\n\nThen open http://localhost:3000 in your browser to access the Dagster UI (Dagit).\n\n### Materializing Assets\n\nIn the Dagit UI, navigate to the Assets tab and select the assets you want to materialize. Click the \"Materialize selected\" button to run the pipeline.\n\n### Scheduling\n\nThe pipeline includes two schedules:\n\n1. `weekly_full_load`: Runs every Sunday at midnight to perform a full load of all data.\n2. `daily_update`: Runs every day at 6 AM to update data for the current day.\n\nEnable these schedules in the Dagit UI to automate the pipeline runs.\n\n## Testing\n\nRun the tests with pytest:\n\n```\npytest\n```\n\n## Data Model\n\nThe Bronze layer contains tables corresponding to the main entities from the ESPN API, including:\n\n- `seasons`: Details for each basketball season\n- `season_types`: Types within a season (e.g., regular, postseason)\n- `weeks`: Weekly breakdown within a season type\n- `events`: Core game information\n- `event_competitors`: Details about teams participating in an event\n- `teams`: General team information\n- `team_details`: Detailed team information\n- `team_roster`: Player rosters for each team\n- `team_statistics`: Aggregated team statistics per game\n- `player_statistics`: Individual player statistics per game\n- `plays`: Detailed log of game events\n- `venues`: Information about game locations\n- `team_records`: Team win/loss records\n- `leaders`: Top player performances in key statistical categories\n- `linescores`: Period-by-period scores\n\n## License\n\n[MIT License](LICENSE)\n```\n\nAlso create additional documentation files:\n\n1. API_ENDPOINTS.md: Detailed documentation of the ESPN API endpoints used in the project\n2. DEVELOPMENT.md: Guide for developers working on the project\n3. TESTING.md: Detailed testing strategy and instructions\n\nAdd docstrings to all Python functions and classes to enable automatic API documentation generation.",
      "testStrategy": "Verify that the documentation is comprehensive, accurate, and follows best practices. Ensure that all key components of the project are documented and that the README provides clear instructions for installation, configuration, and usage.",
      "priority": "medium",
      "dependencies": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16,
        17,
        18,
        19
      ],
      "status": "pending",
      "subtasks": []
    }
  ]
}